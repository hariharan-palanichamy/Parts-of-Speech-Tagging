{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Importing necessary packages</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk,time,random\n",
    "from nltk.tag import map_tag\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Storing Data from NLTK corpus locally into a variable</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_dataset = list(nltk.corpus.treebank.tagged_sents(tagset='universal'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* tagged_dataset - Dataset that is used for training and validating the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Printing first two elements of the dataset to get feel for how the data is stored</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Pierre', 'NOUN'),\n",
       "  ('Vinken', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('61', 'NUM'),\n",
       "  ('years', 'NOUN'),\n",
       "  ('old', 'ADJ'),\n",
       "  (',', '.'),\n",
       "  ('will', 'VERB'),\n",
       "  ('join', 'VERB'),\n",
       "  ('the', 'DET'),\n",
       "  ('board', 'NOUN'),\n",
       "  ('as', 'ADP'),\n",
       "  ('a', 'DET'),\n",
       "  ('nonexecutive', 'ADJ'),\n",
       "  ('director', 'NOUN'),\n",
       "  ('Nov.', 'NOUN'),\n",
       "  ('29', 'NUM'),\n",
       "  ('.', '.')],\n",
       " [('Mr.', 'NOUN'),\n",
       "  ('Vinken', 'NOUN'),\n",
       "  ('is', 'VERB'),\n",
       "  ('chairman', 'NOUN'),\n",
       "  ('of', 'ADP'),\n",
       "  ('Elsevier', 'NOUN'),\n",
       "  ('N.V.', 'NOUN'),\n",
       "  (',', '.'),\n",
       "  ('the', 'DET'),\n",
       "  ('Dutch', 'NOUN'),\n",
       "  ('publishing', 'VERB'),\n",
       "  ('group', 'NOUN'),\n",
       "  ('.', '.')]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_dataset[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Printing total set of POS tags present in the dataset</b><br>\n",
    "Note:- Universal tagset has 12 POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.',\n",
       " 'ADJ',\n",
       " 'ADP',\n",
       " 'ADV',\n",
       " 'CONJ',\n",
       " 'DET',\n",
       " 'NOUN',\n",
       " 'NUM',\n",
       " 'PRON',\n",
       " 'PRT',\n",
       " 'VERB',\n",
       " 'X'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([Tuple[1] for sent in tagged_dataset for Tuple in sent])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Printing count of total unique word present in the dataset</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12408"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set([Tuple[0] for sent in tagged_dataset for Tuple in sent]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Segregating Dataset into test and validation set.</b><br>\n",
    "Note:- Testing is done on data seperately given in the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3718\n",
      "196\n"
     ]
    }
   ],
   "source": [
    "train_set, valid_set = train_test_split(tagged_dataset,test_size=0.05,random_state=20)\n",
    "\n",
    "print(len(train_set))\n",
    "print(len(valid_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* train_set - Data set that is used for training.\n",
    "* valid_set - Data set that is used for validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Converting training data into list of tuples</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tagged_words = [Tuple for sent in train_set for Tuple in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* train_tagged_words - list of (word,POS tag) tuple , obtained from train data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>getting total set of tags</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tagset = set([Tuple[1] for Tuple in train_tagged_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tagset - set of POS tags present in train Data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Converting validation dataset into list of tuples and words</b><br>\n",
    "Note:- Below code is used to select n random sentence from the validation set.By passing 196 for n,I'm selecting all the sentence in validation set in a random order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(20)\n",
    "# choose random 5 sents\n",
    "rndom_num = [random.randint(0,len(valid_set)-1) for x in range(196)]\n",
    "# list of sents\n",
    "select_valid_set = [valid_set[i] for i in rndom_num]\n",
    "# list of tagged words\n",
    "select_valid_tup = [tup for sent in select_valid_set for tup in sent]\n",
    "select_valid_words = [tup[0] for sent in select_valid_set for tup in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* rndom_num - list of n random numbers in validation range\n",
    "* select_valid_set - random sentences chosen from validation set through random numbers present \"rndom_num\" variable\n",
    "* select_valid_tup - list of (word,POS tag) tuple present in validation set.\n",
    "* select_valid_words - list of words present in validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Test dataset hardcoded into the notebook.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set=[['Android','is','a','mobile','operating','system','developed','by','Google','.'],\n",
    "['Android','has','been','the','best-selling','OS','worldwide','on','smartphones','since','2011','and','on','tablets','since','2013','.'],\n",
    "['Google','and','Twitter','made','a','deal','in','2015','that','gave','Google','access','to','Twitter\\'s','firehose','.'],\n",
    "['Twitter','is','an','online','news','and','social','networking','service','on','which','users','post','and','interact','with','messages','known','as','tweets','.'],\n",
    "['Before','entering','politics,','Donald','Trump','was','a','domineering','businessman','and','a','television','personality','.'],\n",
    "['The','2018','FIFA','World','Cup','is','the','21st','FIFA','World','Cup,','an','international','football','tournament','contested','once','every','four','years','.'],\n",
    "['This','is','the','first','World','Cup','to','be','held','in','Eastern','Europe','and','the','11th','time','that','it','has','been','held','in','Europe','.'],\n",
    "['Show','me','the','cheapest','round','trips','from','Dallas','to','Atlanta','.'],\n",
    "['I','would','like','to','see','flights','from','Denver','to','Philadelphia','.'],\n",
    "['Show','me','the','price','of','the','flights','leaving','Atlanta','at','about','3','in','the','afternoon','and','arriving','in','San','Francisco','.'],\n",
    "['NASA','invited','social','media','users','to','experience','the','launch','of','ICESAT-2','Satellite','.']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* test_set - Data set used for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base functions necessary to build HMM based POS taggers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Function that is used to calculate transition probability from tag \"t1\" to \"t2\"</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t2_given_t1(t2, t1, train_bag = train_tagged_words):\n",
    "    #Seperating list of tags in training dataset in order as they appear in sentences.\n",
    "    train_tag_list = [pair[1] for pair in train_bag]\n",
    "    #Getting total count of tag that is t1\n",
    "    count_t1 = len([t for t in train_tag_list if t==t1])\n",
    "    count_t1_t2 = 0\n",
    "    #Getting count of tag t1 that is followed by t2\n",
    "    for index in range(len(train_tag_list)-1):\n",
    "        if train_tag_list[index]==t1 and train_tag_list[index+1] == t2:\n",
    "            count_t1_t2 += 1\n",
    "    return (count_t1_t2, count_t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* t2_given_t1() - Name of function that outputs values which is used to calculate transition probability.\n",
    "* train_tag_list - list of POS tag that is present in training dataset in order as they appear.\n",
    "* count_t1 - count of POS tag t1 present in the dataset.\n",
    "* count_t1_t2 - count of POS tags t1 followed by t2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Function that is used to calculate probability of getting a word given a tag</b><br>\n",
    "Note:- It is also known as emission probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_given_tag(word, tag, train_bag = train_tagged_words):\n",
    "    #getting list of tuples with POS tag that is passed in argument\n",
    "    tup_list_gn_tag = [pair for pair in train_bag if pair[1]==tag]\n",
    "    #count of tuples in the obtained list\n",
    "    tup_count_gn_tag = len(tup_list_gn_tag)\n",
    "    #getting list of tuples where word and POS tag are as passed in argument\n",
    "    wd_list_gn_wd = [pair[0] for pair in tup_list_gn_tag if pair[0]==word]\n",
    "    #count of tuples obtained in the above list\n",
    "    wd_count_gn_wd = len(wd_list_gn_wd)\n",
    "    \n",
    "    return (wd_count_gn_wd, tup_count_gn_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* word_given_tag() - Function that outputs parameters which is used to calculate emission probability.\n",
    "* tup_list_gn_tag - list of tuples whose POS tag is as passed in the function argument\n",
    "* tup_count_gn_tag - count of tuples present in tup_list_gn_tag\n",
    "* wd_list_gn_wd - list of words whose word is as passed in function argument in taglist\n",
    "* wd_count_gn_wd - count of tuple present in wd_list_gn_wd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Generating a matrix that map all transitions from one tag to another tag</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.36009362e-02, 1.74871743e-01, 5.87705895e-02, 2.72702724e-02,\n",
       "        2.20412210e-01, 8.82008821e-02, 6.62406608e-02, 5.22005223e-02,\n",
       "        2.52002524e-03, 9.09909084e-02, 8.09108093e-02, 4.39204387e-02],\n",
       "       [1.70899034e-02, 5.77686867e-03, 4.81405703e-04, 4.59742434e-02,\n",
       "        6.37742221e-01, 3.99566740e-02, 3.73089430e-03, 1.26368999e-02,\n",
       "        2.40702851e-04, 9.26706009e-03, 2.19039600e-02, 2.05199182e-01],\n",
       "       [3.45149264e-02, 1.20802239e-01, 4.66417900e-04, 7.92910438e-03,\n",
       "        3.47947747e-01, 1.56716421e-01, 5.78358211e-02, 5.41044772e-02,\n",
       "        5.13059692e-03, 5.45708947e-02, 4.10447754e-02, 1.18936568e-01],\n",
       "       [1.62231073e-01, 5.52988052e-02, 1.03585655e-02, 7.34661371e-02,\n",
       "        6.15139455e-02, 2.05896407e-01, 5.54581657e-02, 2.59760953e-02,\n",
       "        1.83266938e-01, 1.46454185e-01, 2.86852592e-03, 1.72111560e-02],\n",
       "       [2.39861414e-01, 1.32749816e-02, 4.20131274e-02, 2.90299058e-02,\n",
       "        2.63931423e-01, 1.47410646e-01, 4.81400453e-03, 1.71043035e-02,\n",
       "        4.40189652e-02, 1.76951125e-01, 9.37272049e-03, 1.22173596e-02],\n",
       "       [3.49395722e-02, 1.34102881e-01, 5.50046470e-03, 2.17229620e-01,\n",
       "        1.10009298e-01, 1.69584751e-01, 3.61791141e-02, 8.10350180e-02,\n",
       "        3.17632481e-02, 9.15711224e-02, 2.32414007e-02, 6.48435056e-02],\n",
       "       [4.19687144e-02, 9.91987810e-03, 5.34147257e-03, 9.27127078e-02,\n",
       "        2.11751238e-01, 4.84166354e-01, 8.01220909e-03, 3.24303694e-02,\n",
       "        1.10644791e-02, 2.25104913e-02, 7.24914158e-03, 7.28729516e-02],\n",
       "       [1.36242509e-01, 6.79546967e-02, 7.32844789e-03, 2.33177878e-02,\n",
       "        3.13124582e-02, 3.46435696e-01, 1.53231183e-02, 8.09460357e-02,\n",
       "        1.43237840e-02, 1.19253829e-01, 3.09793465e-02, 1.26582280e-01],\n",
       "       [4.31513563e-02, 1.01667210e-01, 2.28832942e-03, 1.40568810e-02,\n",
       "        2.47793391e-01, 3.98823142e-01, 1.86335407e-02, 9.80712660e-03,\n",
       "        1.96142541e-03, 2.09218692e-02, 5.78620471e-02, 8.30336735e-02],\n",
       "       [3.99361029e-02, 3.23216200e-01, 8.51970166e-04, 3.46112885e-02,\n",
       "        3.23642164e-01, 8.09371658e-03, 6.92225769e-02, 1.34185301e-02,\n",
       "        1.27795525e-03, 1.70394033e-02, 6.27263039e-02, 1.05963789e-01],\n",
       "       [1.13683589e-01, 3.26506374e-03, 1.33570796e-02, 2.09557727e-01,\n",
       "        3.55595142e-01, 1.75126158e-02, 1.48411992e-03, 2.67141592e-03,\n",
       "        2.76046302e-02, 3.56188789e-02, 1.84921339e-01, 3.47284079e-02],\n",
       "       [6.58633187e-02, 4.95212944e-03, 1.68372393e-02, 2.06338726e-02,\n",
       "        6.99735880e-01, 1.20501816e-02, 6.60283898e-04, 4.95212944e-03,\n",
       "        1.12248268e-02, 7.77484328e-02, 2.04688013e-02, 6.48728982e-02]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_trans_matrix = np.zeros((len(Tagset), len(Tagset)), dtype='float32')\n",
    "#Populating matrix with transition probability of moving from one POS tag to another POS tag\n",
    "for i, t1 in enumerate(list(Tagset)):\n",
    "    for j, t2 in enumerate(list(Tagset)): \n",
    "        #transition probability is calculated as (count of tag t1 followed by t2) / (count of tag t1)\n",
    "        tags_trans_matrix[i, j] = t2_given_t1(t2, t1)[0]/t2_given_t1(t2, t1)[1]\n",
    "\n",
    "tags_trans_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* tags_matix - matrix used to store transition probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Converting the mapping matrix into a DataFrame</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>.</th>\n",
       "      <th>DET</th>\n",
       "      <th>CONJ</th>\n",
       "      <th>X</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>VERB</th>\n",
       "      <th>PRON</th>\n",
       "      <th>ADV</th>\n",
       "      <th>PRT</th>\n",
       "      <th>ADP</th>\n",
       "      <th>NUM</th>\n",
       "      <th>ADJ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>0.093601</td>\n",
       "      <td>0.174872</td>\n",
       "      <td>0.058771</td>\n",
       "      <td>0.027270</td>\n",
       "      <td>0.220412</td>\n",
       "      <td>0.088201</td>\n",
       "      <td>0.066241</td>\n",
       "      <td>0.052201</td>\n",
       "      <td>0.002520</td>\n",
       "      <td>0.090991</td>\n",
       "      <td>0.080911</td>\n",
       "      <td>0.043920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DET</th>\n",
       "      <td>0.017090</td>\n",
       "      <td>0.005777</td>\n",
       "      <td>0.000481</td>\n",
       "      <td>0.045974</td>\n",
       "      <td>0.637742</td>\n",
       "      <td>0.039957</td>\n",
       "      <td>0.003731</td>\n",
       "      <td>0.012637</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.009267</td>\n",
       "      <td>0.021904</td>\n",
       "      <td>0.205199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CONJ</th>\n",
       "      <td>0.034515</td>\n",
       "      <td>0.120802</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.007929</td>\n",
       "      <td>0.347948</td>\n",
       "      <td>0.156716</td>\n",
       "      <td>0.057836</td>\n",
       "      <td>0.054104</td>\n",
       "      <td>0.005131</td>\n",
       "      <td>0.054571</td>\n",
       "      <td>0.041045</td>\n",
       "      <td>0.118937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X</th>\n",
       "      <td>0.162231</td>\n",
       "      <td>0.055299</td>\n",
       "      <td>0.010359</td>\n",
       "      <td>0.073466</td>\n",
       "      <td>0.061514</td>\n",
       "      <td>0.205896</td>\n",
       "      <td>0.055458</td>\n",
       "      <td>0.025976</td>\n",
       "      <td>0.183267</td>\n",
       "      <td>0.146454</td>\n",
       "      <td>0.002869</td>\n",
       "      <td>0.017211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOUN</th>\n",
       "      <td>0.239861</td>\n",
       "      <td>0.013275</td>\n",
       "      <td>0.042013</td>\n",
       "      <td>0.029030</td>\n",
       "      <td>0.263931</td>\n",
       "      <td>0.147411</td>\n",
       "      <td>0.004814</td>\n",
       "      <td>0.017104</td>\n",
       "      <td>0.044019</td>\n",
       "      <td>0.176951</td>\n",
       "      <td>0.009373</td>\n",
       "      <td>0.012217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VERB</th>\n",
       "      <td>0.034940</td>\n",
       "      <td>0.134103</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.217230</td>\n",
       "      <td>0.110009</td>\n",
       "      <td>0.169585</td>\n",
       "      <td>0.036179</td>\n",
       "      <td>0.081035</td>\n",
       "      <td>0.031763</td>\n",
       "      <td>0.091571</td>\n",
       "      <td>0.023241</td>\n",
       "      <td>0.064844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRON</th>\n",
       "      <td>0.041969</td>\n",
       "      <td>0.009920</td>\n",
       "      <td>0.005341</td>\n",
       "      <td>0.092713</td>\n",
       "      <td>0.211751</td>\n",
       "      <td>0.484166</td>\n",
       "      <td>0.008012</td>\n",
       "      <td>0.032430</td>\n",
       "      <td>0.011064</td>\n",
       "      <td>0.022510</td>\n",
       "      <td>0.007249</td>\n",
       "      <td>0.072873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADV</th>\n",
       "      <td>0.136243</td>\n",
       "      <td>0.067955</td>\n",
       "      <td>0.007328</td>\n",
       "      <td>0.023318</td>\n",
       "      <td>0.031312</td>\n",
       "      <td>0.346436</td>\n",
       "      <td>0.015323</td>\n",
       "      <td>0.080946</td>\n",
       "      <td>0.014324</td>\n",
       "      <td>0.119254</td>\n",
       "      <td>0.030979</td>\n",
       "      <td>0.126582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRT</th>\n",
       "      <td>0.043151</td>\n",
       "      <td>0.101667</td>\n",
       "      <td>0.002288</td>\n",
       "      <td>0.014057</td>\n",
       "      <td>0.247793</td>\n",
       "      <td>0.398823</td>\n",
       "      <td>0.018634</td>\n",
       "      <td>0.009807</td>\n",
       "      <td>0.001961</td>\n",
       "      <td>0.020922</td>\n",
       "      <td>0.057862</td>\n",
       "      <td>0.083034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADP</th>\n",
       "      <td>0.039936</td>\n",
       "      <td>0.323216</td>\n",
       "      <td>0.000852</td>\n",
       "      <td>0.034611</td>\n",
       "      <td>0.323642</td>\n",
       "      <td>0.008094</td>\n",
       "      <td>0.069223</td>\n",
       "      <td>0.013419</td>\n",
       "      <td>0.001278</td>\n",
       "      <td>0.017039</td>\n",
       "      <td>0.062726</td>\n",
       "      <td>0.105964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUM</th>\n",
       "      <td>0.113684</td>\n",
       "      <td>0.003265</td>\n",
       "      <td>0.013357</td>\n",
       "      <td>0.209558</td>\n",
       "      <td>0.355595</td>\n",
       "      <td>0.017513</td>\n",
       "      <td>0.001484</td>\n",
       "      <td>0.002671</td>\n",
       "      <td>0.027605</td>\n",
       "      <td>0.035619</td>\n",
       "      <td>0.184921</td>\n",
       "      <td>0.034728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADJ</th>\n",
       "      <td>0.065863</td>\n",
       "      <td>0.004952</td>\n",
       "      <td>0.016837</td>\n",
       "      <td>0.020634</td>\n",
       "      <td>0.699736</td>\n",
       "      <td>0.012050</td>\n",
       "      <td>0.000660</td>\n",
       "      <td>0.004952</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>0.077748</td>\n",
       "      <td>0.020469</td>\n",
       "      <td>0.064873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             .       DET      CONJ         X      NOUN      VERB      PRON  \\\n",
       ".     0.093601  0.174872  0.058771  0.027270  0.220412  0.088201  0.066241   \n",
       "DET   0.017090  0.005777  0.000481  0.045974  0.637742  0.039957  0.003731   \n",
       "CONJ  0.034515  0.120802  0.000466  0.007929  0.347948  0.156716  0.057836   \n",
       "X     0.162231  0.055299  0.010359  0.073466  0.061514  0.205896  0.055458   \n",
       "NOUN  0.239861  0.013275  0.042013  0.029030  0.263931  0.147411  0.004814   \n",
       "VERB  0.034940  0.134103  0.005500  0.217230  0.110009  0.169585  0.036179   \n",
       "PRON  0.041969  0.009920  0.005341  0.092713  0.211751  0.484166  0.008012   \n",
       "ADV   0.136243  0.067955  0.007328  0.023318  0.031312  0.346436  0.015323   \n",
       "PRT   0.043151  0.101667  0.002288  0.014057  0.247793  0.398823  0.018634   \n",
       "ADP   0.039936  0.323216  0.000852  0.034611  0.323642  0.008094  0.069223   \n",
       "NUM   0.113684  0.003265  0.013357  0.209558  0.355595  0.017513  0.001484   \n",
       "ADJ   0.065863  0.004952  0.016837  0.020634  0.699736  0.012050  0.000660   \n",
       "\n",
       "           ADV       PRT       ADP       NUM       ADJ  \n",
       ".     0.052201  0.002520  0.090991  0.080911  0.043920  \n",
       "DET   0.012637  0.000241  0.009267  0.021904  0.205199  \n",
       "CONJ  0.054104  0.005131  0.054571  0.041045  0.118937  \n",
       "X     0.025976  0.183267  0.146454  0.002869  0.017211  \n",
       "NOUN  0.017104  0.044019  0.176951  0.009373  0.012217  \n",
       "VERB  0.081035  0.031763  0.091571  0.023241  0.064844  \n",
       "PRON  0.032430  0.011064  0.022510  0.007249  0.072873  \n",
       "ADV   0.080946  0.014324  0.119254  0.030979  0.126582  \n",
       "PRT   0.009807  0.001961  0.020922  0.057862  0.083034  \n",
       "ADP   0.013419  0.001278  0.017039  0.062726  0.105964  \n",
       "NUM   0.002671  0.027605  0.035619  0.184921  0.034728  \n",
       "ADJ   0.004952  0.011225  0.077748  0.020469  0.064873  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_trans_df = pd.DataFrame(tags_trans_matrix, columns = list(Tagset), index=list(Tagset))\n",
    "tags_trans_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* tags_trans_df - DataFrame which is created from the matrix tags_trans_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plain Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Plain Viterbi function implementation without handling any unknown words.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plain_Viterbi(words, train_bag = train_tagged_words):\n",
    "    pred_tag = []\n",
    "    Tagset = list(set([pair[1] for pair in train_bag]))\n",
    "    #Below for loop parses through list of all words sequentially.\n",
    "    for key, word in enumerate(words):\n",
    "        resultant_prob_lst = []\n",
    "        #Below for loop parses through list of available tagset.\n",
    "        for tag in Tagset:\n",
    "            #Transition Probability is calculated between two tags using Data Frame generated previously.\n",
    "            #For the starting word there is no intial tag available.so we assign tag \".\" which \n",
    "            #indicates starting of the sentence.\n",
    "            if key == 0:\n",
    "                trans_prob = tags_trans_df.loc['.', tag]\n",
    "            else:\n",
    "                trans_prob = tags_trans_df.loc[pred_tag[-1], tag]\n",
    "                \n",
    "            # Computing Emission probability\n",
    "            emission_count,emission_tot=word_given_tag(words[key], tag)\n",
    "            emission_prob = emission_count/emission_tot\n",
    "            # State Probability is the product of emission and transition probability.\n",
    "            resultant_prob = emission_prob * trans_prob\n",
    "            resultant_prob_lst.append(resultant_prob)    \n",
    "        max_prob = max(resultant_prob_lst)\n",
    "        \n",
    "        #Selecting next tag that has maximum probability of occuring.\n",
    "        max_prob_tag = Tagset[resultant_prob_lst.index(max_prob)] \n",
    "        pred_tag.append(max_prob_tag)\n",
    "    return list(zip(words, pred_tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plain_Viterbi() - Function that implements plain viterbi algorithm.\n",
    "* pred_tag - variable used to store sequence of POS tags for the passed word sequence.\n",
    "* resultant_prob_lst - variable containing list of probability for next POS tag.\n",
    "* trans_prob -  variable that stores trasitional probability.\n",
    "* emission_prob -  variable that stores emission probability.\n",
    "* emission_count - variable that stores count of word given tag in the training data set.\n",
    "* emission_tot - variable that stores specified tag count in training data set.\n",
    "* resultant_prob - resultant probability of a tag for the next word.\n",
    "* max_prob - value of maximum probability from the list of resultant_prob.\n",
    "* max_prob_tag - variable that stores tag which has maximum probability for the next word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Predicting tag for words by plain viterbi algorithm for words in validation set</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in seconds:  558.8093910217285\n"
     ]
    }
   ],
   "source": [
    "#Tagging the POS tags for validation set\n",
    "start_time = time.time()\n",
    "tagged_word_seq = Plain_Viterbi(select_valid_words)\n",
    "end_time = time.time()\n",
    "time_diff = end_time-start_time\n",
    "print(\"Time taken in seconds: \", time_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* start_time - starting time of prediction.\n",
    "* end_time - ending time of prediction.\n",
    "* time_diff - time taken for the prediction to complete.\n",
    "* tagged_word_seq - validation word sequence which has been tagged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Calculating Accuracy of plain viterbi algorithm for words in validation sets</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.903539639826123"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting list of tuples whose actual and predicted value matches\n",
    "correct_pred_list = [i for i, j in zip(tagged_word_seq, select_valid_tup) if i == j] \n",
    "#Accuracy is count of tuple that matches / count of total tuple\n",
    "accuracy = len(correct_pred_list)/len(tagged_word_seq)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* correct_pred_list - list of (words,POS tag) tuple that matches with acutual value\n",
    "* accuracy - accuracy of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>POS tagging for test set</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Android', '.'), ('is', 'VERB'), ('a', 'DET'), ('mobile', 'ADJ'), ('operating', 'NOUN'), ('system', 'NOUN'), ('developed', 'VERB'), ('by', 'ADP'), ('Google', '.'), ('.', '.')]\n",
      "\n",
      "\n",
      "\n",
      "[('Android', '.'), ('has', 'VERB'), ('been', 'VERB'), ('the', 'DET'), ('best-selling', 'ADJ'), ('OS', '.'), ('worldwide', '.'), ('on', 'ADP'), ('smartphones', '.'), ('since', 'ADP'), ('2011', '.'), ('and', 'CONJ'), ('on', 'ADP'), ('tablets', 'NOUN'), ('since', 'ADP'), ('2013', '.'), ('.', '.')]\n",
      "\n",
      "\n",
      "\n",
      "[('Google', '.'), ('and', 'CONJ'), ('Twitter', '.'), ('made', 'VERB'), ('a', 'DET'), ('deal', 'NOUN'), ('in', 'ADP'), ('2015', '.'), ('that', 'DET'), ('gave', 'VERB'), ('Google', '.'), ('access', 'NOUN'), ('to', 'PRT'), (\"Twitter's\", '.'), ('firehose', '.'), ('.', '.')]\n",
      "\n",
      "\n",
      "\n",
      "[('Twitter', '.'), ('is', 'VERB'), ('an', 'DET'), ('online', '.'), ('news', 'NOUN'), ('and', 'CONJ'), ('social', 'ADJ'), ('networking', 'NOUN'), ('service', 'NOUN'), ('on', 'ADP'), ('which', 'DET'), ('users', 'NOUN'), ('post', 'NOUN'), ('and', 'CONJ'), ('interact', '.'), ('with', 'ADP'), ('messages', '.'), ('known', 'VERB'), ('as', 'ADP'), ('tweets', '.'), ('.', '.')]\n",
      "\n",
      "\n",
      "\n",
      "[('Before', 'ADP'), ('entering', 'VERB'), ('politics,', '.'), ('Donald', 'NOUN'), ('Trump', 'NOUN'), ('was', 'VERB'), ('a', 'DET'), ('domineering', '.'), ('businessman', 'NOUN'), ('and', 'CONJ'), ('a', 'DET'), ('television', 'NOUN'), ('personality', '.'), ('.', '.')]\n",
      "\n",
      "\n",
      "\n",
      "[('The', 'DET'), ('2018', '.'), ('FIFA', '.'), ('World', 'NOUN'), ('Cup', '.'), ('is', 'VERB'), ('the', 'DET'), ('21st', '.'), ('FIFA', '.'), ('World', 'NOUN'), ('Cup,', '.'), ('an', 'DET'), ('international', 'ADJ'), ('football', 'NOUN'), ('tournament', '.'), ('contested', '.'), ('once', 'ADV'), ('every', 'DET'), ('four', 'NUM'), ('years', 'NOUN'), ('.', '.')]\n",
      "\n",
      "\n",
      "\n",
      "[('This', 'DET'), ('is', 'VERB'), ('the', 'DET'), ('first', 'ADJ'), ('World', 'NOUN'), ('Cup', '.'), ('to', 'PRT'), ('be', 'VERB'), ('held', 'VERB'), ('in', 'ADP'), ('Eastern', 'NOUN'), ('Europe', 'NOUN'), ('and', 'CONJ'), ('the', 'DET'), ('11th', 'ADJ'), ('time', 'NOUN'), ('that', 'ADP'), ('it', 'PRON'), ('has', 'VERB'), ('been', 'VERB'), ('held', 'VERB'), ('in', 'ADP'), ('Europe', 'NOUN'), ('.', '.')]\n",
      "\n",
      "\n",
      "\n",
      "[('Show', 'NOUN'), ('me', 'PRON'), ('the', 'DET'), ('cheapest', 'ADJ'), ('round', 'NOUN'), ('trips', '.'), ('from', 'ADP'), ('Dallas', 'NOUN'), ('to', 'PRT'), ('Atlanta', 'NOUN'), ('.', '.')]\n",
      "\n",
      "\n",
      "\n",
      "[('I', 'PRON'), ('would', 'VERB'), ('like', 'ADP'), ('to', 'PRT'), ('see', 'VERB'), ('flights', 'NOUN'), ('from', 'ADP'), ('Denver', 'NOUN'), ('to', 'PRT'), ('Philadelphia', 'NOUN'), ('.', '.')]\n",
      "\n",
      "\n",
      "\n",
      "[('Show', 'NOUN'), ('me', 'PRON'), ('the', 'DET'), ('price', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('flights', 'NOUN'), ('leaving', 'VERB'), ('Atlanta', 'NOUN'), ('at', 'ADP'), ('about', 'ADP'), ('3', 'NUM'), ('in', 'ADP'), ('the', 'DET'), ('afternoon', 'NOUN'), ('and', 'CONJ'), ('arriving', '.'), ('in', 'ADP'), ('San', 'NOUN'), ('Francisco', 'NOUN'), ('.', '.')]\n",
      "\n",
      "\n",
      "\n",
      "[('NASA', '.'), ('invited', '.'), ('social', 'ADJ'), ('media', 'NOUN'), ('users', 'NOUN'), ('to', 'PRT'), ('experience', 'NOUN'), ('the', 'DET'), ('launch', 'NOUN'), ('of', 'ADP'), ('ICESAT-2', '.'), ('Satellite', '.'), ('.', '.')]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(test_set)):\n",
    "    print(str(Plain_Viterbi(test_set[i])))\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1 : Viterbi with Rule based Algorithm to handle unknown words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Creating rule based tagger, along with patterns that is used to calculate the tags</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [\n",
    "    (r'.*ing$', 'VBG'),              # gerund\n",
    "    (r'.*ed$', 'VBD'),               # past tense\n",
    "    (r'.*es$', 'VBZ'),               # 3rd singular present\n",
    "    (r'.*ould$', 'MD'),              # modals\n",
    "    (r'.*\\'s$', 'NN$'),              # possessive nouns\n",
    "    (r'.*s$', 'NNS'),                # plural nouns\n",
    "    (r'^-?[0-9]+(.[0-9]+)?$', 'CD'), # cardinal numbers\n",
    "    (r'.*', 'NN')                    # nouns\n",
    "]\n",
    "\n",
    "\n",
    "#Rule based tagger\n",
    "rule_based_tagger = nltk.RegexpTagger(patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pattern - list of pattern to be checked by rule based tagger.\n",
    "* rule_based_tagger - regextagger object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Viterbi function implementation that handles unknown words by rule based tagging</b><br><br>\n",
    "Note:- In this algorithm if a word is unknown,instead of tagging a random tag for unkown word ,we use rule based tagging method.Please see the comments inside code for understanding of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Rule_Viterbi(words, train_bag = train_tagged_words):\n",
    "    pred_tag = []\n",
    "    Tagset = list(set([pair[1] for pair in train_bag]))\n",
    "    #Below for loop parses through list of all words sequentially.\n",
    "    for key, word in enumerate(words):\n",
    "        resultant_prob_lst = []\n",
    "        #Below for loop parses through list of available tagset\n",
    "        for tag in Tagset:\n",
    "            #Transition Probability is calculated between two tags.\n",
    "            #For the starting word there is no intial tag available.so we assign tag \".\" which indicates starting of the sentence.\n",
    "            if key == 0:\n",
    "                trans_prob = tags_trans_df.loc['.', tag]\n",
    "            else:\n",
    "                trans_prob = tags_trans_df.loc[pred_tag[-1], tag]\n",
    "                \n",
    "            # Computing Emission probability\n",
    "            emission_count,emission_tot=word_given_tag(words[key], tag)\n",
    "            emission_prob = emission_count/emission_tot\n",
    "            # State Probability is the product of emission and transition probability.\n",
    "            resultant_prob = emission_prob * trans_prob\n",
    "            resultant_prob_lst.append(resultant_prob)\n",
    "        \n",
    "        max_prob = max(resultant_prob_lst)\n",
    "        #If max_prob=0 , it means word is unknown ,then we use rule based tagger as given in else condition.\n",
    "        if(max_prob!=0.0):\n",
    "            #Selecting next tag that has maximum probability of occuring.\n",
    "            max_prob_tag = Tagset[resultant_prob_lst.index(max_prob)] \n",
    "        else:\n",
    "            #map_tag() is used to map normal POS tagset to universal tagset.\n",
    "            max_prob_tag=map_tag('en-ptb', 'universal',rule_based_tagger.tag([word])[0][1])\n",
    "        pred_tag.append(max_prob_tag)\n",
    "    return list(zip(words, pred_tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Predicting tag for words by viterbi algorithm,handling unknown words by rule based tagging</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in seconds:  513.1577730178833\n"
     ]
    }
   ],
   "source": [
    "#Tagging the POS tags for validation set\n",
    "start_time = time.time()\n",
    "tagged_word_seq = Rule_Viterbi(select_valid_words)\n",
    "end_time = time.time()\n",
    "time_diff = end_time-start_time\n",
    "print(\"Time taken in seconds: \", time_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Calculating Accuracy of viterbi algorithm,along with rule based tagging for handling unknown words </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9503208445456427"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting list of tuples whose actual and predicted value matches\n",
    "correct_pred_list = [i for i, j in zip(tagged_word_seq, select_valid_tup) if i == j]\n",
    "#Accuracy is count of tuple that matches / count of total tuple\n",
    "accuracy = len(correct_pred_list)/len(tagged_word_seq)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Android', 'NOUN'), ('is', 'VERB'), ('a', 'DET'), ('mobile', 'ADJ'), ('operating', 'NOUN'), ('system', 'NOUN'), ('developed', 'VERB'), ('by', 'ADP'), ('Google', 'NOUN'), ('.', '.')]\n",
      "\n",
      "\n",
      "\n",
      "[('Android', 'NOUN'), ('has', 'VERB'), ('been', 'VERB'), ('the', 'DET'), ('best-selling', 'ADJ'), ('OS', 'NOUN'), ('worldwide', 'NOUN'), ('on', 'ADP'), ('smartphones', 'VERB'), ('since', 'ADP'), ('2011', 'NUM'), ('and', 'CONJ'), ('on', 'ADP'), ('tablets', 'NOUN'), ('since', 'ADP'), ('2013', 'NUM'), ('.', '.')]\n",
      "\n",
      "\n",
      "\n",
      "[('Google', 'NOUN'), ('and', 'CONJ'), ('Twitter', 'NOUN'), ('made', 'VERB'), ('a', 'DET'), ('deal', 'NOUN'), ('in', 'ADP'), ('2015', 'NUM'), ('that', 'ADP'), ('gave', 'VERB'), ('Google', 'NOUN'), ('access', 'NOUN'), ('to', 'PRT'), (\"Twitter's\", 'X'), ('firehose', 'NOUN'), ('.', '.')]\n",
      "\n",
      "\n",
      "\n",
      "[('Twitter', 'NOUN'), ('is', 'VERB'), ('an', 'DET'), ('online', 'NOUN'), ('news', 'NOUN'), ('and', 'CONJ'), ('social', 'ADJ'), ('networking', 'NOUN'), ('service', 'NOUN'), ('on', 'ADP'), ('which', 'DET'), ('users', 'NOUN'), ('post', 'NOUN'), ('and', 'CONJ'), ('interact', 'NOUN'), ('with', 'ADP'), ('messages', 'VERB'), ('known', 'VERB'), ('as', 'ADP'), ('tweets', 'NOUN'), ('.', '.')]\n",
      "\n",
      "\n",
      "\n",
      "[('Before', 'ADP'), ('entering', 'VERB'), ('politics,', 'NOUN'), ('Donald', 'NOUN'), ('Trump', 'NOUN'), ('was', 'VERB'), ('a', 'DET'), ('domineering', 'VERB'), ('businessman', 'NOUN'), ('and', 'CONJ'), ('a', 'DET'), ('television', 'NOUN'), ('personality', 'NOUN'), ('.', '.')]\n",
      "\n",
      "\n",
      "\n",
      "[('The', 'DET'), ('2018', 'NUM'), ('FIFA', 'NOUN'), ('World', 'NOUN'), ('Cup', 'NOUN'), ('is', 'VERB'), ('the', 'DET'), ('21st', 'NOUN'), ('FIFA', 'NOUN'), ('World', 'NOUN'), ('Cup,', 'NOUN'), ('an', 'DET'), ('international', 'ADJ'), ('football', 'NOUN'), ('tournament', 'NOUN'), ('contested', 'VERB'), ('once', 'ADV'), ('every', 'DET'), ('four', 'NUM'), ('years', 'NOUN'), ('.', '.')]\n",
      "\n",
      "\n",
      "\n",
      "[('This', 'DET'), ('is', 'VERB'), ('the', 'DET'), ('first', 'ADJ'), ('World', 'NOUN'), ('Cup', 'NOUN'), ('to', 'PRT'), ('be', 'VERB'), ('held', 'VERB'), ('in', 'ADP'), ('Eastern', 'NOUN'), ('Europe', 'NOUN'), ('and', 'CONJ'), ('the', 'DET'), ('11th', 'ADJ'), ('time', 'NOUN'), ('that', 'ADP'), ('it', 'PRON'), ('has', 'VERB'), ('been', 'VERB'), ('held', 'VERB'), ('in', 'ADP'), ('Europe', 'NOUN'), ('.', '.')]\n",
      "\n",
      "\n",
      "\n",
      "[('Show', 'NOUN'), ('me', 'PRON'), ('the', 'DET'), ('cheapest', 'ADJ'), ('round', 'NOUN'), ('trips', 'NOUN'), ('from', 'ADP'), ('Dallas', 'NOUN'), ('to', 'PRT'), ('Atlanta', 'NOUN'), ('.', '.')]\n",
      "\n",
      "\n",
      "\n",
      "[('I', 'PRON'), ('would', 'VERB'), ('like', 'ADP'), ('to', 'PRT'), ('see', 'VERB'), ('flights', 'NOUN'), ('from', 'ADP'), ('Denver', 'NOUN'), ('to', 'PRT'), ('Philadelphia', 'NOUN'), ('.', '.')]\n",
      "\n",
      "\n",
      "\n",
      "[('Show', 'NOUN'), ('me', 'PRON'), ('the', 'DET'), ('price', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('flights', 'NOUN'), ('leaving', 'VERB'), ('Atlanta', 'NOUN'), ('at', 'ADP'), ('about', 'ADP'), ('3', 'NUM'), ('in', 'ADP'), ('the', 'DET'), ('afternoon', 'NOUN'), ('and', 'CONJ'), ('arriving', 'VERB'), ('in', 'ADP'), ('San', 'NOUN'), ('Francisco', 'NOUN'), ('.', '.')]\n",
      "\n",
      "\n",
      "\n",
      "[('NASA', 'NOUN'), ('invited', 'VERB'), ('social', 'ADJ'), ('media', 'NOUN'), ('users', 'NOUN'), ('to', 'PRT'), ('experience', 'NOUN'), ('the', 'DET'), ('launch', 'NOUN'), ('of', 'ADP'), ('ICESAT-2', 'NOUN'), ('Satellite', 'NOUN'), ('.', '.')]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(test_set)):\n",
    "    print(str(Rule_Viterbi(test_set[i])))\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2:- Viterbi with Probabilistic method to handle unkown words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>viterbi algorithm where unkown words are handled by probabilistic approach</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prob_Viterbi(words, train_bag = train_tagged_words):\n",
    "    pred_tag = []\n",
    "    Tagset = list(set([pair[1] for pair in train_bag]))\n",
    "    #Below for loop parses through list of all words sequentially.\n",
    "    for key, word in enumerate(words):\n",
    "        resultant_prob_lst = []\n",
    "        Trans_prob=[]\n",
    "        #Below for loop parses through list of available tagset\n",
    "        for tag in Tagset:\n",
    "            #Transition Probability is calculated between two tags.\n",
    "            #For the starting word there is no intial tag available.so we assign tag \".\" which indicates starting of the sentence.\n",
    "            if key == 0:\n",
    "                trans_prob = tags_trans_df.loc['.', tag]\n",
    "            else:\n",
    "                trans_prob = tags_trans_df.loc[pred_tag[-1], tag]\n",
    "                \n",
    "            # Computing Emission probability\n",
    "            emission_count,emission_tot=word_given_tag(words[key], tag)\n",
    "            emission_prob = emission_count/emission_tot\n",
    "            # State Probability is the product of emission and transition probability.\n",
    "            resultant_prob = emission_prob * trans_prob\n",
    "            resultant_prob_lst.append(resultant_prob)\n",
    "            Trans_prob.append(trans_prob)\n",
    "\n",
    "        max_prob = max(resultant_prob_lst)\n",
    "        max_prob_tag = Tagset[resultant_prob_lst.index(max_prob)]\n",
    "        #If emission probability is 0,then we consider only transition probability to decide next POS tag\n",
    "        if(max_prob==0.0):\n",
    "            max_prob = max(Trans_prob)\n",
    "            max_prob_tag = Tagset[Trans_prob.index(max_prob)]\n",
    "        pred_tag.append(max_prob_tag)\n",
    "    return list(zip(words, pred_tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trans_prob - variable containing list of probability of next POS tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Predicting tag for words by viterbi algorithm,handling unknown words by probabilistic tagging</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in seconds:  514.9112617969513\n"
     ]
    }
   ],
   "source": [
    "#Tagging the POS tags for validation set\n",
    "start_time = time.time()\n",
    "tagged_word_seq = Prob_Viterbi(select_valid_words)\n",
    "end_time = time.time()\n",
    "time_diff = end_time-start_time\n",
    "print(\"Time taken in seconds: \", time_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Calculating Accuracy of viterbi algorithm,along with probabilistic tagging for unknown words </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9389360380873525"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting list of tuples whose actual and predicted value matches\n",
    "correct_pred_list = [i for i, j in zip(tagged_word_seq, select_valid_tup) if i == j]\n",
    "#Accuracy is count of tuple that matches / count of total tuple\n",
    "accuracy = len(correct_pred_list)/len(tagged_word_seq)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Android', 'NOUN'), ('is', 'VERB'), ('a', 'DET'), ('mobile', 'ADJ'), ('operating', 'NOUN'), ('system', 'NOUN'), ('developed', 'VERB'), ('by', 'ADP'), ('Google', 'NOUN'), ('.', '.')]\n",
      "\n",
      "\n",
      "\n",
      "[('Android', 'NOUN'), ('has', 'VERB'), ('been', 'VERB'), ('the', 'DET'), ('best-selling', 'ADJ'), ('OS', 'NOUN'), ('worldwide', 'NOUN'), ('on', 'ADP'), ('smartphones', 'NOUN'), ('since', 'ADP'), ('2011', 'NOUN'), ('and', 'CONJ'), ('on', 'ADP'), ('tablets', 'NOUN'), ('since', 'ADP'), ('2013', 'NOUN'), ('.', '.')]\n",
      "\n",
      "\n",
      "\n",
      "[('Google', 'NOUN'), ('and', 'CONJ'), ('Twitter', 'NOUN'), ('made', 'VERB'), ('a', 'DET'), ('deal', 'NOUN'), ('in', 'ADP'), ('2015', 'NOUN'), ('that', 'ADP'), ('gave', 'VERB'), ('Google', 'X'), ('access', 'NOUN'), ('to', 'PRT'), (\"Twitter's\", 'VERB'), ('firehose', 'X'), ('.', '.')]\n",
      "\n",
      "\n",
      "\n",
      "[('Twitter', 'NOUN'), ('is', 'VERB'), ('an', 'DET'), ('online', 'NOUN'), ('news', 'NOUN'), ('and', 'CONJ'), ('social', 'ADJ'), ('networking', 'NOUN'), ('service', 'NOUN'), ('on', 'ADP'), ('which', 'DET'), ('users', 'NOUN'), ('post', 'NOUN'), ('and', 'CONJ'), ('interact', 'NOUN'), ('with', 'ADP'), ('messages', 'NOUN'), ('known', 'VERB'), ('as', 'ADP'), ('tweets', 'NOUN'), ('.', '.')]\n",
      "\n",
      "\n",
      "\n",
      "[('Before', 'ADP'), ('entering', 'VERB'), ('politics,', 'X'), ('Donald', 'NOUN'), ('Trump', 'NOUN'), ('was', 'VERB'), ('a', 'DET'), ('domineering', 'NOUN'), ('businessman', 'NOUN'), ('and', 'CONJ'), ('a', 'DET'), ('television', 'NOUN'), ('personality', 'NOUN'), ('.', '.')]\n",
      "\n",
      "\n",
      "\n",
      "[('The', 'DET'), ('2018', 'NOUN'), ('FIFA', 'NOUN'), ('World', 'NOUN'), ('Cup', 'NOUN'), ('is', 'VERB'), ('the', 'DET'), ('21st', 'NOUN'), ('FIFA', 'NOUN'), ('World', 'NOUN'), ('Cup,', 'NOUN'), ('an', 'DET'), ('international', 'ADJ'), ('football', 'NOUN'), ('tournament', 'NOUN'), ('contested', 'NOUN'), ('once', 'ADV'), ('every', 'DET'), ('four', 'NUM'), ('years', 'NOUN'), ('.', '.')]\n",
      "\n",
      "\n",
      "\n",
      "[('This', 'DET'), ('is', 'VERB'), ('the', 'DET'), ('first', 'ADJ'), ('World', 'NOUN'), ('Cup', 'NOUN'), ('to', 'PRT'), ('be', 'VERB'), ('held', 'VERB'), ('in', 'ADP'), ('Eastern', 'NOUN'), ('Europe', 'NOUN'), ('and', 'CONJ'), ('the', 'DET'), ('11th', 'ADJ'), ('time', 'NOUN'), ('that', 'ADP'), ('it', 'PRON'), ('has', 'VERB'), ('been', 'VERB'), ('held', 'VERB'), ('in', 'ADP'), ('Europe', 'NOUN'), ('.', '.')]\n",
      "\n",
      "\n",
      "\n",
      "[('Show', 'NOUN'), ('me', 'PRON'), ('the', 'DET'), ('cheapest', 'ADJ'), ('round', 'NOUN'), ('trips', 'NOUN'), ('from', 'ADP'), ('Dallas', 'NOUN'), ('to', 'PRT'), ('Atlanta', 'NOUN'), ('.', '.')]\n",
      "\n",
      "\n",
      "\n",
      "[('I', 'PRON'), ('would', 'VERB'), ('like', 'ADP'), ('to', 'PRT'), ('see', 'VERB'), ('flights', 'NOUN'), ('from', 'ADP'), ('Denver', 'NOUN'), ('to', 'PRT'), ('Philadelphia', 'NOUN'), ('.', '.')]\n",
      "\n",
      "\n",
      "\n",
      "[('Show', 'NOUN'), ('me', 'PRON'), ('the', 'DET'), ('price', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('flights', 'NOUN'), ('leaving', 'VERB'), ('Atlanta', 'NOUN'), ('at', 'ADP'), ('about', 'ADP'), ('3', 'NUM'), ('in', 'ADP'), ('the', 'DET'), ('afternoon', 'NOUN'), ('and', 'CONJ'), ('arriving', 'NOUN'), ('in', 'ADP'), ('San', 'NOUN'), ('Francisco', 'NOUN'), ('.', '.')]\n",
      "\n",
      "\n",
      "\n",
      "[('NASA', 'NOUN'), ('invited', 'NOUN'), ('social', 'ADJ'), ('media', 'NOUN'), ('users', 'NOUN'), ('to', 'PRT'), ('experience', 'NOUN'), ('the', 'DET'), ('launch', 'NOUN'), ('of', 'ADP'), ('ICESAT-2', 'NOUN'), ('Satellite', 'NOUN'), ('.', '.')]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(test_set)):\n",
    "    print(str(Prob_Viterbi(test_set[i])))\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result:-\n",
    "\n",
    "* <b>Accuracy of plain Viterbi algorithm:- 90.35%</b>\n",
    "* <b>Accuracy of Viterbi algorithm with rule based method to handle unknown words:- 95.03%</b>\n",
    "* <b>Accuracy of Viterbi algorithm with probabilistic method to handle unknown words:- 93.89%</b>\n",
    "\n",
    "\n",
    "<b>Cases where words are correctly tagged for viterbi along with rule based algorithm:-</b>\n",
    "\n",
    "* Android - Conj to Noun \n",
    "* google - Conj to Noun\n",
    "* os - Conj to Noun \n",
    "* worldwide - Conj to Noun \n",
    "* 2011 - Conj to Num \n",
    "* 2013 - Conj to Num \n",
    "* twitter - Conj to Noun \n",
    "* online - Conj to Noun \n",
    "* tweets - Conj to Noun\n",
    "* politics - Conj to Noun \n",
    "* domineering - Conj to Verb\n",
    "* fifa - Conj to Noun\n",
    "* cup - Conj to Noun\n",
    "* 21st - Conj to Noun\n",
    "* tournament - Conj to Noun\n",
    "* contested - Conj to Verb\n",
    "* arriving - Conj to Verb\n",
    "* Nasa - Conj to Noun\n",
    "* icesat-2 - Conj to Noun\n",
    "* satellite - Conj to Noun \n",
    "* invited - Conj to Verb\n",
    "\n",
    "\n",
    "<b>Cases where words are correctly tagged for viterbi along with probabilistic algorithm:-</b>\n",
    "\n",
    "* Android - Conj to Noun\n",
    "* google - Conj to Noun\n",
    "* os - Conj to Noun\n",
    "* worldwide - Conj to Noun\n",
    "* 2013 - Conj to Num\n",
    "* smartphones - Conj to Noun\n",
    "* twitter - Conj to Noun\n",
    "* online - Conj to Noun\n",
    "* tweets - Conj to Noun\n",
    "* personality - Conj to Noun\n",
    "* fifa - Conj to Noun\n",
    "* cup - Conj to Noun\n",
    "* tournament - Conj to Noun\n",
    "* Trips - Conj to Noun\n",
    "* Nasa - Conj to Noun\n",
    "* icesat-2 - Conj to Noun\n",
    "* satellite - Conj to Noun"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
